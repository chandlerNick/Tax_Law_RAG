{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb53674",
   "metadata": {},
   "source": [
    "# Evaluate FT BERT\n",
    "\n",
    "In this notebook, we fine tune BERT on a document classification task, compare the performance against the pretrained BERT (also with a trained linear projection from the cls token to the 11 classes on the same hyperparameters but frozen BERT parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb6169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DL-data/.dsw_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "\n",
    "NS = {'uslm': 'http://xml.house.gov/schemas/uslm/1.0',\n",
    "      'xhtml': 'http://www.w3.org/1999/xhtml'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8354e1a",
   "metadata": {},
   "source": [
    "Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274fdaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ancestor_heading_text(section, tag, ns):\n",
    "    ancestor = section.getparent()\n",
    "    while ancestor is not None:\n",
    "        if ancestor.tag == f\"{{{ns['uslm']}}}{tag}\":\n",
    "            heading = ancestor.find('uslm:heading', namespaces=ns)\n",
    "            return heading.text.strip() if heading is not None else \"\"\n",
    "        ancestor = ancestor.getparent()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def parse_sections_with_metadata(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        tree = etree.parse(f)\n",
    "    \n",
    "    sections = tree.findall('.//uslm:section', namespaces=NS)\n",
    "    parsed = []\n",
    "\n",
    "    for section in sections:\n",
    "        heading = section.find('uslm:heading', namespaces=NS)\n",
    "        heading_text = heading.text.strip() if heading is not None else \"\"\n",
    "\n",
    "        # Get all paragraphs (and any nested elements)\n",
    "        content_texts = []\n",
    "        for p in section.findall('.//uslm:p', namespaces=NS):\n",
    "            text = ' '.join(p.itertext()).strip()\n",
    "            if text:\n",
    "                content_texts.append(text)\n",
    "\n",
    "        if len(content_texts) == 0:\n",
    "            continue\n",
    "\n",
    "        # Get ancestors: subtitle, chapter, part\n",
    "        subtitle = get_ancestor_heading_text(section, 'subtitle', NS)\n",
    "        chapter = get_ancestor_heading_text(section, 'chapter', NS)\n",
    "        part = get_ancestor_heading_text(section, 'part', NS)\n",
    "\n",
    "        parsed.append({\n",
    "            \"metadata\": {\n",
    "                \"subtitle\": subtitle,\n",
    "                \"chapter\": chapter,\n",
    "                \"part\": part\n",
    "                },\n",
    "            \"content\": \"\\n\".join(content_texts)\n",
    "        })\n",
    "\n",
    "    return parsed\n",
    "\n",
    "\n",
    "class SubtitleDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b768d",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e4a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSubtitleClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, freeze_bert=False):\n",
    "            super().__init__()\n",
    "            self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "            if freeze_bert:\n",
    "                for param in self.bert.parameters():\n",
    "                    param.requires_grad = False  # Disable autograd so we can compare the finetuned to pretrained\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "            self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775dd06d",
   "metadata": {},
   "source": [
    "Define Training and Eval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "695a71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    print(f\"  âž¤ Accuracy: {acc:.4f} | Macro-F1: {f1_macro:.4f}\")\n",
    "    return f1_macro  # Return F1 for cross-validation comparison\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, val_loader, optimizer, criterion, device, epochs=5, patience=2):\n",
    "    best_f1 = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Training step\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\" Avg train loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation step\n",
    "        f1_macro = evaluate(model, val_loader, device)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if f1_macro > best_f1:\n",
    "            best_f1 = f1_macro\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "            print(f\"  ðŸŽ‰ New best Macro-F1: {best_f1:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  No improvement for {epochs_no_improve} epochs.\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Stopping early after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    # Load best weights before returning\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_final_model(parsed_data, num_epochs, batch_size, lr, patience=2, device=None, freeze_bert=False, save_dir=\"bert-subtitle-embedder\"):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Extract data\n",
    "    texts = [entry[\"content\"] for entry in parsed_data]\n",
    "    subtitles = [entry[\"metadata\"][\"subtitle\"] for entry in parsed_data]\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(subtitles)\n",
    "\n",
    "    # Train-val split (90/10) for early stopping\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        texts, labels, test_size=0.1, stratify=labels, random_state=42\n",
    "    )\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Dataset and loaders\n",
    "    train_dataset = SubtitleDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = SubtitleDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Init model\n",
    "    model = BertSubtitleClassifier(num_labels=len(label_encoder.classes_)).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train with early stopping\n",
    "    model = train_with_early_stopping(\n",
    "        model, train_loader, val_loader, optimizer, criterion, device,\n",
    "        epochs=num_epochs, patience=patience\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\nFinal Evaluation:\\n\")\n",
    "    evaluate(model, val_loader, device)\n",
    "\n",
    "    # Save only fine-tuned BERT encoder if not frozen\n",
    "    if not freeze_bert:\n",
    "        model.bert.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"Fine-tuned model saved to {save_dir}\")\n",
    "    else:\n",
    "        print(\"Frozen BERT model trained and evaluated (not saved).\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519da05",
   "metadata": {},
   "source": [
    "Call train and use best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d695e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 3e-05\n",
    "epochs = 4\n",
    "batch_size = 8\n",
    "patience = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "save_dir = \"/DL-data/Tax_Law_RAG/usc26/USC26_Subtitle_Classification_BERT\"\n",
    "\n",
    "# Data\n",
    "parsed_data = parse_sections_with_metadata(\"/DL-data/usc26.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7762bf0",
   "metadata": {},
   "source": [
    "Evaluate BERT and FT BERT on F1 and Accuracy - Note stdout\n",
    "\n",
    "def train_final_model(parsed_data, num_epochs, batch_size, lr, patience=2, device=None, freeze_bert=False, save_dir=\"bert-subtitle-embedder\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909049d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 239/239 [01:28<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg train loss: 1.1627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:07<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âž¤ Accuracy: 0.7512 | Macro-F1: 0.4776\n",
      "  ðŸŽ‰ New best Macro-F1: 0.4776\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 239/239 [01:22<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg train loss: 0.4820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:06<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âž¤ Accuracy: 0.8169 | Macro-F1: 0.5962\n",
      "  ðŸŽ‰ New best Macro-F1: 0.5962\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 111/239 [00:39<00:45,  2.82it/s]"
     ]
    }
   ],
   "source": [
    "# Train Fine Tuned BERT\n",
    "train_final_model(parsed_data=parsed_data, num_epochs=epochs, batch_size=batch_size, lr=lr, patience=patience, device=device, freeze_bert=False, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ebeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier (linear projection) on frozen BERT\n",
    "train_final_model(parsed_data=parsed_data, num_epochs=epochs, batch_size=batch_size, lr=lr, patience=patience, device=device, freeze_bert=True, save_dir=save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dsw_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
